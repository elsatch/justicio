{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General imports\n",
    "\n",
    "from llama_index.core.llama_dataset import LabelledRagDataset\n",
    "from openai import OpenAI\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: e:\\dev\\justicio\\src\\etls\\boe\\loading\n"
     ]
    }
   ],
   "source": [
    "# General variables and data import\n",
    "print(f'Current working directory: {os.getcwd()}')\n",
    "\n",
    "# IF using OpenAI, insert your OpenAI API key here\n",
    "client = OpenAI(\n",
    "    base_url = 'http://192.168.1.220:11434/v1',\n",
    "    api_key='ollama', # required, but unused\n",
    ")\n",
    "\n",
    "# Default model config\n",
    "model = 'eas/nous-hermes-2-solar-10.7b:q8_0'\n",
    "\n",
    "test_dir = os.path.join('test')\n",
    "\n",
    "rag_dataset = LabelledRagDataset.from_json(\n",
    "    os.path.join(test_dir, 'rag_test_justicio_dataset.json')\n",
    ")\n",
    "\n",
    "rag_dataset_df = rag_dataset.to_pandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comment to work on the whole dataset\n",
    "rag_dataset_df = rag_dataset_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To save space, we eliminate the reference_context columns since we are only evaluating the questions\n",
    "\n",
    "rag_dataset_df = rag_dataset_df.drop(columns=['reference_contexts'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation functions\n",
    "\n",
    "# Function to rate how fit is the question for a legal professional\n",
    "def rate_pro_fit_question(question):\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {'role': 'system', 'content': 'You are a helpful assistant, that reponses using a single digit from 1 to 10.'},\n",
    "            {\n",
    "                'role': 'user',\n",
    "                'content': f'Rate the following question on a scale of 1 to 10 based on its relevance and usefulness for a legal professional with experience in legal databases in Spain. Consider complexity, specificity, and practical value. Return one of the following values [1, 2, 3, 4, 5], without any additional comments or explanations. Question: {question}',\n",
    "            },\n",
    "        ],\n",
    "    )\n",
    "    # Assuming the response will be a single digit number in the text.\n",
    "    try:\n",
    "        rating = response.choices[0].message.content.strip()\n",
    "    except ValueError:\n",
    "        # In case the response is not a number, default to NaN\n",
    "        rating = 'NaN'\n",
    "    return rating\n",
    "\n",
    "# Function to rate if this is question about data or metadata (Note: True/False works better than data/metadata, which tends to add artifacts to the output)\n",
    "def rate_data_metadata_question(question):\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {'role': 'system', 'content': 'You are a helpful assistant, that especializes in data classification.'},\n",
    "            {\n",
    "                'role': 'user',\n",
    "                'content': f\"\"\"Classify the following question considering whether is deals with data on the document or with metadata information. \n",
    "                \n",
    "                Questions about data might include:\n",
    "                - Which article of the GDPR is about data protection?\n",
    "                - When was this law dictated?\n",
    "                - What department was in charge of publishing this law?\n",
    "                - When was this law approved?\n",
    "\n",
    "                Questions about metadata might include: \n",
    "                - What is the name of the file?\n",
    "                - When was the file created?\n",
    "\n",
    "                If the question provided deals with data return the word \"True\". Otherwise, return the word \"False\". Do not add any additional comments or explanations. \n",
    "                Question: {question}\"\"\",\n",
    "            },\n",
    "        ],\n",
    "    )\n",
    "    # Assuming the response will be a single digit number in the text.\n",
    "    try:\n",
    "        rating = response.choices[0].message.content.strip()\n",
    "    except ValueError:\n",
    "        # In case the response is not a number, default to NaN\n",
    "        rating = 'NaN'\n",
    "    return rating\n",
    "\n",
    "# Function to evaluate if this is a quick search question\n",
    "def rate_quick_search_question(question):\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {'role': 'system', 'content': 'You are a helpful assistant, that especializes in data classification.'},\n",
    "            {\n",
    "                'role': 'user',\n",
    "                'content': f\"\"\"Classify the following question considering whether it is a quick search question or not. \n",
    "                \n",
    "                Quick search questions follow the following pattern \"artículo xxx de la ley\" like:\n",
    "                - Artículo 12 de la Ley de Propiedad Horizontal\n",
    "                - Artículo primero del Código Civil\n",
    "                - Párrafo 3 del artículo 5 de la Ley de Enjuiciamiento Civil\n",
    "\n",
    "                If the question provided is a quick search question, return \"True\". Otherwise, return the \"False\". Do not add any additional comments or explanations. \n",
    "                Question: {question}\"\"\",\n",
    "            },\n",
    "        ],\n",
    "    )\n",
    "    # Assuming the response will be a single digit number in the text.\n",
    "    try:\n",
    "        rating = response.choices[0].message.content.strip()\n",
    "    except ValueError:\n",
    "        # In case the response is not a number, default to NaN\n",
    "        rating = 'NaN'\n",
    "    return rating\n",
    "\n",
    "# Function to evaluate if this is a quick search question\n",
    "def rate_direct_law_question(question):\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {'role': 'system', 'content': 'You are a helpful assistant, that especializes in data classification.'},\n",
    "            {\n",
    "                'role': 'user',\n",
    "                'content': f\"\"\"Classify the following question considering whether it is a direct law question or not. \n",
    "                \n",
    "                Direct law questions are meant to know recent changes on issues that are modified frequently.\n",
    "\n",
    "                These are several examples of direct law questions:\n",
    "                - How many ministries are currently in Spain?\n",
    "                - What is the current minimum wage in Spain?\n",
    "                - What is the current VAT in Spain for plane tickets?\n",
    "                \n",
    "                If the question provided is a direct law question, return \"True\". Otherwise, return the \"False\". Do not add any additional comments or explanations. \n",
    "                Question: {question}\"\"\",\n",
    "            },\n",
    "        ],\n",
    "    )\n",
    "    # Assuming the response will be a single digit number in the text.\n",
    "    try:\n",
    "        rating = response.choices[0].message.content.strip()\n",
    "    except ValueError:\n",
    "        # In case the response is not a number, default to NaN\n",
    "        rating = 'NaN'\n",
    "    return rating\n",
    "\n",
    "# Function to rate how fit is the question for a legal professional\n",
    "def rate_multiple_sources_question(question):\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {'role': 'system', 'content': 'You are a helpful assistant, that especializes in data categorization.'},\n",
    "            {\n",
    "                'role': 'user',\n",
    "                'content': f\"\"\"Rate the following question considering whether it deals with multiple sources of information or not.\n",
    "\n",
    "                Multiple sources questions are usually asked when preparing a case, a client response, or a legal document. They are meant to gather information from different sources to build a solid argument. \n",
    "                \n",
    "                If the question provided is a question requiring multiple sources, return \"True\". Otherwise, return the \"False\". Do not add any additional comments or explanations.\n",
    "                Question: {question}\"\"\",\n",
    "            },\n",
    "        ],\n",
    "    )\n",
    "    # Assuming the response will be a single digit number in the text.\n",
    "    try:\n",
    "        rating = response.choices[0].message.content.strip()\n",
    "    except ValueError:\n",
    "        # In case the response is not a number, default to NaN\n",
    "        rating = 'NaN'\n",
    "    return rating\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a complete evaluation\n",
    "\n",
    "def rate_questions(rag_dataset_df, num_runs, model):\n",
    "    rag_dataset_df['run'] = num_runs\n",
    "    rag_dataset_df['is_fit_for_pro'] = rag_dataset_df['query'].apply(rate_pro_fit_question)\n",
    "    rag_dataset_df['is_data_or_metadata'] = rag_dataset_df['query'].apply(rate_data_metadata_question)\n",
    "    rag_dataset_df['is_quick_search'] = rag_dataset_df['query'].apply(rate_quick_search_question)\n",
    "    rag_dataset_df['is_direct_law'] = rag_dataset_df['query'].apply(rate_direct_law_question)\n",
    "    rag_dataset_df['is_multiple_sources'] = rag_dataset_df['query'].apply(rate_multiple_sources_question)\n",
    "    rag_dataset_df['model'] = model\n",
    "    return rag_dataset_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               query  run is_fit_for_pro  \\\n",
      "0  En qué fecha se actualizó la información conte...    1              4   \n",
      "1  ¿Cuál es el origen legislativo del documento c...    1              7   \n",
      "2  A qué departamento del gobierno pertenece la p...    1              7   \n",
      "3  ¿Cuál es el tipo de rango asignado al document...    1              7   \n",
      "4  En qué fecha se dictó la disposición contenida...    1              7   \n",
      "\n",
      "  is_data_or_metadata is_quick_search is_direct_law is_multiple_sources  \\\n",
      "0                True           False          True               False   \n",
      "1                True           False          True                True   \n",
      "2                True           False         False                True   \n",
      "3                True           False          True               False   \n",
      "4               False           False         False               False   \n",
      "\n",
      "                                model  \n",
      "0  eas/nous-hermes-2-solar-10.7b:q8_0  \n",
      "1  eas/nous-hermes-2-solar-10.7b:q8_0  \n",
      "2  eas/nous-hermes-2-solar-10.7b:q8_0  \n",
      "3  eas/nous-hermes-2-solar-10.7b:q8_0  \n",
      "4  eas/nous-hermes-2-solar-10.7b:q8_0  \n"
     ]
    }
   ],
   "source": [
    "# Let's generate multiple runs for a single model\n",
    "test_runs = 2\n",
    "\n",
    "# Community model names include / and ., so we need to sanitize the model name\n",
    "sanitized_model = model.replace('/', '_').replace('.', '_').replace(':', '_')\n",
    "\n",
    "# First run\n",
    "rag_dataset_df = rate_questions(rag_dataset_df, 0, model)\n",
    "rag_dataset_df.to_csv(os.path.join(test_dir, f'rag_dataset_evaluations_{sanitized_model}.csv'), mode='w', index=True, header=True)\n",
    "\n",
    "# Subsequent runs\n",
    "for i in range(1, test_runs):\n",
    "    rag_dataset_df = rate_questions(rag_dataset_df, i, model)\n",
    "    print(rag_dataset_df[['query', 'run', 'is_fit_for_pro', 'is_data_or_metadata', 'is_quick_search', 'is_direct_law', 'is_multiple_sources', 'model']])\n",
    "    # Append the result of all runs to a single file\n",
    "    rag_dataset_df.to_csv(os.path.join(test_dir, f'rag_dataset_evaluations_{sanitized_model}.csv'), mode='a', header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, let's generate multiple runs for multiple models\n",
    "num_runs = 10\n",
    "\n",
    "models = ['eas/nous-hermes-2-solar-10.7b:q8_0', 'cas/nous-hermes-2-mistral-7b-dpo', 'openhermes2.5-mistral', 'neural-chat:7b-v3.3-q5_K_M', 'macadeliccc/laser-dolphin-mixtral-2x7b-dpo'] #, 'ifioravanti/alphamonarch']\n",
    "\n",
    "for model in models:\n",
    "    sanitized_model = model.replace('/', '_').replace('.', '_').replace(':', '_')\n",
    "\n",
    "    rag_dataset_df = rate_questions(rag_dataset_df, 0, model)\n",
    "    rag_dataset_df.to_csv(os.path.join(test_dir, f'rag_dataset_evaluations_{sanitized_model}.csv'), mode='w', index=True, header=True)\n",
    "    \n",
    "    for i in range(1, num_runs):\n",
    "        rag_dataset_df = rate_questions(rag_dataset_df, i, model)\n",
    "        print(rag_dataset_df[['query', 'run', 'is_fit_for_pro', 'is_data_or_metadata', 'is_quick_search', 'is_direct_law', 'is_multiple_sources', 'model']])\n",
    "        rag_dataset_df.to_csv(os.path.join(test_dir, f'rag_dataset_evaluations_{sanitized_model}.csv'), mode='a', header=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
